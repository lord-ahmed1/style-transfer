{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ee4b6a",
   "metadata": {},
   "source": [
    "# Neural Style Transfer Demonstration\n",
    "\n",
    "This notebook demonstrates the neural style transfer technique implemented in our project. We'll showcase:\n",
    "1. Basic style transfer using the VGG19 CNN model\n",
    "2. The effect of different style thresholds\n",
    "3. Comparison with Vision Transformer (ViT) implementation\n",
    "4. Analysis of loss functions during optimization\n",
    "\n",
    "Let's begin by importing the necessary modules and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import project modules\n",
    "from config.config import *\n",
    "from utils.image_utils import image_loader, save_image, display_images, get_file_paths\n",
    "from utils.optimizer import run_optimization\n",
    "from models.model_factory import get_model\n",
    "from losses.content_loss import ContentLoss\n",
    "from losses.style_loss import StyleLoss\n",
    "from losses.tv_loss import TotalVariationLoss\n",
    "\n",
    "# Ensure we're using GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1677cf",
   "metadata": {},
   "source": [
    "## 1. Loading the Images\n",
    "\n",
    "Let's load a content image and a style image to use for our demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d66e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available content and style images\n",
    "content_paths, style_paths = get_file_paths(CONTENT_DIR, STYLE_DIR)\n",
    "\n",
    "# Print available images\n",
    "print(\"Available content images:\")\n",
    "for i, path in enumerate(content_paths):\n",
    "    print(f\"[{i}] {os.path.basename(path)}\")\n",
    "\n",
    "print(\"\\nAvailable style images:\")\n",
    "for i, path in enumerate(style_paths):\n",
    "    print(f\"[{i}] {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed418cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select indices of content and style images\n",
    "content_idx = 0  # Change this to select a different content image\n",
    "style_idx = 0    # Change this to select a different style image\n",
    "\n",
    "# Load the selected images\n",
    "content_path = content_paths[content_idx]\n",
    "style_path = style_paths[style_idx]\n",
    "\n",
    "print(f\"Selected content image: {os.path.basename(content_path)}\")\n",
    "print(f\"Selected style image: {os.path.basename(style_path)}\")\n",
    "\n",
    "# Load images\n",
    "content_img = image_loader(content_path, IMAGE_SIZE, device)\n",
    "style_img = image_loader(style_path, IMAGE_SIZE, device)\n",
    "\n",
    "# Display the original images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Convert tensors to numpy arrays for display\n",
    "def tensor_to_np_img(tensor):\n",
    "    img = tensor.cpu().clone().detach().numpy().squeeze(0)\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "ax1.imshow(tensor_to_np_img(content_img))\n",
    "ax1.set_title('Content Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(tensor_to_np_img(style_img))\n",
    "ax2.set_title('Style Image')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618bf66",
   "metadata": {},
   "source": [
    "## 2. Basic Style Transfer with VGG19\n",
    "\n",
    "Now, let's perform style transfer using the VGG19-based model with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fa5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_style_transfer(content_img, style_img, model_type='vgg', style_threshold=0.7,\n",
    "                          num_steps=300, style_weight=1e6, content_weight=1, tv_weight=1,\n",
    "                          content_layers=None, style_layers=None):\n",
    "    \"\"\"Function to perform neural style transfer.\"\"\"\n",
    "    # Use default layers if not specified\n",
    "    if content_layers is None:\n",
    "        content_layers = CONTENT_LAYERS\n",
    "    if style_layers is None:\n",
    "        style_layers = STYLE_LAYERS\n",
    "    \n",
    "    # Initialize model\n",
    "    model = get_model(model_type, content_layers, style_layers, device)\n",
    "    \n",
    "    # Create a white noise image as the starting point\n",
    "    input_img = content_img.clone().requires_grad_(True)\n",
    "    \n",
    "    # Extract features\n",
    "    content_features, _ = model(content_img)\n",
    "    _, style_features = model(style_img)\n",
    "    \n",
    "    # Create loss modules\n",
    "    content_losses = [ContentLoss(f.detach()).to(device) for f in content_features]\n",
    "    \n",
    "    style_losses = []\n",
    "    for i, f in enumerate(style_features):\n",
    "        # Layer weighting\n",
    "        layer_weight = (i + 1)**2\n",
    "        style_loss = StyleLoss(f.detach(), layer_weight, style_threshold).to(device)\n",
    "        style_losses.append(style_loss)\n",
    "    \n",
    "    # Add total variation loss\n",
    "    tv_loss = TotalVariationLoss(weight=tv_weight).to(device)\n",
    "    \n",
    "    # Run optimization\n",
    "    output_img, loss_history = run_optimization(\n",
    "        model, input_img, content_losses, style_losses,\n",
    "        tv_loss, num_steps, style_weight, content_weight, tv_weight, style_threshold\n",
    "    )\n",
    "    \n",
    "    return output_img, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbdbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform style transfer with VGG19\n",
    "print(\"Running style transfer with VGG19...\")\n",
    "output_img_vgg, loss_history_vgg = perform_style_transfer(\n",
    "    content_img, style_img,\n",
    "    model_type='vgg',\n",
    "    style_threshold=0.7,\n",
    "    num_steps=300,  # Using fewer steps for demonstration\n",
    "    style_weight=1e6,\n",
    "    content_weight=1,\n",
    "    tv_weight=1\n",
    ")\n",
    "\n",
    "# Display the resulting image alongside originals\n",
    "fig = display_images(content_img, style_img, output_img_vgg)\n",
    "plt.title(\"VGG19-based Style Transfer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd7893",
   "metadata": {},
   "source": [
    "## 3. Analyzing the Effect of Style Threshold\n",
    "\n",
    "Let's explore how different style thresholds affect the output. The style threshold controls how much of the style is applied to the content image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different style thresholds\n",
    "thresholds = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "outputs = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(f\"Running style transfer with threshold = {threshold}...\")\n",
    "    output_img, _ = perform_style_transfer(\n",
    "        content_img, style_img, \n",
    "        style_threshold=threshold,\n",
    "        num_steps=200  # Fewer steps for faster demonstration\n",
    "    )\n",
    "    outputs.append(output_img)\n",
    "\n",
    "# Display all results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Show content and style images\n",
    "axes[0, 0].imshow(tensor_to_np_img(content_img))\n",
    "axes[0, 0].set_title('Content Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(tensor_to_np_img(style_img))\n",
    "axes[0, 1].set_title('Style Image')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Hide one subplot if there are only 5 thresholds\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Show results with different thresholds\n",
    "for i, (threshold, output) in enumerate(zip(thresholds, outputs)):\n",
    "    row = 1 if i >= 3 else 0\n",
    "    col = i % 3 if i < 3 else i - 3\n",
    "    axes[row, col].imshow(tensor_to_np_img(output))\n",
    "    axes[row, col].set_title(f'Threshold = {threshold}')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2077e",
   "metadata": {},
   "source": [
    "## 4. Using Vision Transformer (ViT) [Bonus]\n",
    "\n",
    "Let's try style transfer using a Vision Transformer model instead of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ac062",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if transformers library is installed\n",
    "    import transformers\n",
    "    print(\"Running style transfer with Vision Transformer...\")\n",
    "    output_img_vit, loss_history_vit = perform_style_transfer(\n",
    "        content_img, style_img,\n",
    "        model_type='vit',\n",
    "        style_threshold=0.7,\n",
    "        num_steps=300,  # Using fewer steps for demonstration\n",
    "        style_weight=1e5,  # Different weights may work better for ViT\n",
    "        content_weight=10,\n",
    "        tv_weight=1\n",
    "    )\n",
    "    \n",
    "    # Compare VGG and ViT results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    \n",
    "    axes[0, 0].imshow(tensor_to_np_img(content_img))\n",
    "    axes[0, 0].set_title('Content Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(tensor_to_np_img(style_img))\n",
    "    axes[0, 1].set_title('Style Image')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(tensor_to_np_img(output_img_vgg))\n",
    "    axes[1, 0].set_title('VGG19 Result')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(tensor_to_np_img(output_img_vit))\n",
    "    axes[1, 1].set_title('Vision Transformer Result')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"The 'transformers' library is not installed. Install it with:\")\n",
    "    print(\"pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e30fd1",
   "metadata": {},
   "source": [
    "## 5. Loss Analysis\n",
    "\n",
    "Let's analyze the loss values during the optimization process to understand the convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history from our VGG model run\n",
    "plt.figure(figsize=(10, 6))\n",
    "steps = range(1, len(loss_history_vgg['total'])+1)\n",
    "\n",
    "plt.plot(steps, loss_history_vgg['content'], label='Content Loss')\n",
    "plt.plot(steps, loss_history_vgg['style'], label='Style Loss')\n",
    "plt.plot(steps, loss_history_vgg['tv'], label='Total Variation Loss')\n",
    "plt.plot(steps, loss_history_vgg['total'], label='Total Loss')\n",
    "\n",
    "plt.xlabel('Optimization Steps')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Loss History during Style Transfer Optimization')\n",
    "plt.legend()\n",
    "plt.yscale('log')  # Log scale to see small changes\n",
    "plt.grid(True, which=\"both\", ls=\"--\", c='.75')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7896e9",
   "metadata": {},
   "source": [
    "## 6. Saving the Results\n",
    "\n",
    "Let's save our generated images to the results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86449b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the VGG result\n",
    "content_name = os.path.basename(content_path).split('.')[0]\n",
    "style_name = os.path.basename(style_path).split('.')[0]\n",
    "output_path = os.path.join(RESULTS_DIR, f\"{content_name}_{style_name}_vgg.jpg\")\n",
    "save_image(output_img_vgg, output_path)\n",
    "print(f\"VGG result saved to {output_path}\")\n",
    "\n",
    "# Save the comparison figure\n",
    "fig = display_images(content_img, style_img, output_img_vgg)\n",
    "comparison_path = os.path.join(RESULTS_DIR, f\"{content_name}_{style_name}_comparison.jpg\")\n",
    "plt.savefig(comparison_path)\n",
    "print(f\"Comparison image saved to {comparison_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
